#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gemini API é«˜çº§åŠŸèƒ½åº”ç”¨
æ”¯æŒå¤šè½®å¯¹è¯ã€å‚æ•°è°ƒèŠ‚ã€å¤šç§åº”ç”¨åœºæ™¯
"""

import google.generativeai as genai
import os
import json
from datetime import datetime
from typing import List, Dict, Optional

# åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

class GeminiAdvanced:
    def __init__(self, api_key: str = None, model_name: str = None):
        """åˆå§‹åŒ– Gemini é«˜çº§å®¢æˆ·ç«¯"""
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("è¯·è®¾ç½® GOOGLE_API_KEY ç¯å¢ƒå˜é‡æˆ–ä¼ å…¥ api_key å‚æ•°")
        
        genai.configure(api_key=self.api_key)
        
        # ä» .env æ–‡ä»¶è¯»å–é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        self.model_name = model_name or os.getenv("DEFAULT_MODEL", "gemini-1.5-flash")
        
        # ä» .env æ–‡ä»¶è¯»å–å®‰å…¨è®¾ç½®é…ç½®
        safety_level = os.getenv("SAFETY_LEVEL", "BLOCK_MEDIUM_AND_ABOVE")
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": safety_level
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": safety_level
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": safety_level
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": safety_level
            }
        ]
        
        self.model = genai.GenerativeModel(
            self.model_name,
            safety_settings=self.safety_settings
        )
        self.chat_history: List[Dict] = []
        
        # ä» .env æ–‡ä»¶è¯»å–ç”Ÿæˆé…ç½®
        self.default_config = {
            "temperature": float(os.getenv("DEFAULT_TEMPERATURE", "0.7")),
            "top_p": float(os.getenv("DEFAULT_TOP_P", "0.8")),
            "top_k": int(os.getenv("DEFAULT_TOP_K", "40")),
            "max_output_tokens": int(os.getenv("DEFAULT_MAX_TOKENS", "1000")),
        }
    
    def show_config(self):
        """æ˜¾ç¤ºå½“å‰é…ç½®"""
        print("âš™ï¸ å½“å‰ Gemini é…ç½®ï¼š")
        print(f"   ğŸ¤– APIå¯†é’¥ï¼š{self.api_key[:8]}..." if self.api_key else "   âŒ æœªè®¾ç½®APIå¯†é’¥")
        print(f"   ğŸ§  æ¨¡å‹åç§°ï¼š{self.model_name}")
        print(f"   ğŸŒ¡ï¸ æ¸©åº¦å‚æ•°ï¼š{self.default_config['temperature']}")
        print(f"   ğŸ“ æœ€å¤§è¾“å‡ºï¼š{self.default_config['max_output_tokens']} tokens")
        print(f"   ğŸ”§ Top-pï¼š{self.default_config['top_p']}")
        print(f"   ğŸ”§ Top-kï¼š{self.default_config['top_k']}")
        print(f"   ğŸ’­ å¯¹è¯å†å²ï¼š{len(self.chat_history)} æ¡è®°å½•")
    
    def list_available_models(self):
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
        print("ğŸ¤– å¯ç”¨çš„ Gemini æ¨¡å‹ï¼š")
        for model in genai.list_models():
            if 'generateContent' in model.supported_generation_methods:
                print(f"   â€¢ {model.name}")
    
    def generate_text(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        config = {**self.default_config, **kwargs}
        
        try:
            response = self.model.generate_content(
                prompt,
                generation_config=config
            )
            
            # è¯¦ç»†çš„å“åº”æ£€æŸ¥å’Œé”™è¯¯å¤„ç†
            if not response.candidates:
                return "âŒ ç”Ÿæˆå¤±è´¥ï¼šæ²¡æœ‰è¿”å›å€™é€‰å“åº”"
            
            candidate = response.candidates[0]
            
            # æ£€æŸ¥ finish_reason
            if candidate.finish_reason == 1:  # STOP - æ­£å¸¸å®Œæˆ
                if candidate.content and candidate.content.parts:
                    return response.text
                else:
                    return "âŒ ç”Ÿæˆå¤±è´¥ï¼šå“åº”å†…å®¹ä¸ºç©º"
            elif candidate.finish_reason == 2:  # MAX_TOKENS
                return "âŒ ç”Ÿæˆå¤±è´¥ï¼šè¾¾åˆ°æœ€å¤§tokené™åˆ¶ï¼Œè¯·å¢åŠ  max_output_tokens"
            elif candidate.finish_reason == 3:  # SAFETY
                return "âŒ ç”Ÿæˆå¤±è´¥ï¼šå†…å®¹è¢«å®‰å…¨è¿‡æ»¤å™¨é˜»æ­¢ï¼Œè¯·å°è¯•ä¿®æ”¹æç¤ºè¯"
            elif candidate.finish_reason == 4:  # RECITATION
                return "âŒ ç”Ÿæˆå¤±è´¥ï¼šæ£€æµ‹åˆ°é‡å¤å†…å®¹"
            else:
                return f"âŒ ç”Ÿæˆå¤±è´¥ï¼šæœªçŸ¥çš„finish_reason: {candidate.finish_reason}"
                
        except Exception as e:
            return f"âŒ ç”Ÿæˆå¤±è´¥ï¼š{e}"
    
    def start_chat(self, system_prompt: str = None):
        """å¼€å§‹èŠå¤©ä¼šè¯"""
        if system_prompt:
            self.chat_history = [{"role": "system", "content": system_prompt}]
        else:
            self.chat_history = []
        
        chat = self.model.start_chat(history=[])
        return chat
    
    def chat_with_history(self, message: str, chat_session=None) -> str:
        """å¸¦å†å²è®°å½•çš„èŠå¤©"""
        if chat_session is None:
            chat_session = self.model.start_chat(history=[])
        
        try:
            response = chat_session.send_message(message)
            
            # è®°å½•å¯¹è¯å†å²
            self.chat_history.append({
                "timestamp": datetime.now().isoformat(),
                "user": message,
                "assistant": response.text
            })
            
            return response.text
        except Exception as e:
            return f"âŒ å¯¹è¯å¤±è´¥ï¼š{e}"
    
    def save_chat_history(self, filename: str = None):
        """ä¿å­˜å¯¹è¯å†å²"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"chat_history_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.chat_history, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ å¯¹è¯å†å²å·²ä¿å­˜åˆ°ï¼š{filename}")
    
    def load_chat_history(self, filename: str):
        """åŠ è½½å¯¹è¯å†å²"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                self.chat_history = json.load(f)
            print(f"ğŸ“‚ å¯¹è¯å†å²å·²ä» {filename} åŠ è½½")
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
        except json.JSONDecodeError:
            print(f"âŒ æ–‡ä»¶ {filename} æ ¼å¼é”™è¯¯")
    
    def analyze_image(self, image_path: str, prompt: str = "æè¿°è¿™å¼ å›¾ç‰‡") -> str:
        """åˆ†æå›¾ç‰‡"""
        try:
            from PIL import Image
            
            # æ‰“å¼€å›¾ç‰‡
            img = Image.open(image_path)
            
            # ä½¿ç”¨æ”¯æŒè§†è§‰çš„æ¨¡å‹
            vision_model = genai.GenerativeModel('gemini-1.5-flash')
            response = vision_model.generate_content([prompt, img])
            
            return response.text
        except ImportError:
            return "âŒ è¯·å®‰è£… Pillow åº“ï¼špip install Pillow"
        except Exception as e:
            return f"âŒ å›¾ç‰‡åˆ†æå¤±è´¥ï¼š{e}"
    
    def translate_text(self, text: str, target_language: str = "ä¸­æ–‡") -> str:
        """ç¿»è¯‘æ–‡æœ¬"""
        prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼š\n\n{text}"
        return self.generate_text(prompt)
    
    def summarize_text(self, text: str, max_length: int = 200) -> str:
        """æ–‡æœ¬æ‘˜è¦"""
        prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬æ€»ç»“ä¸ºä¸è¶…è¿‡{max_length}å­—çš„æ‘˜è¦ï¼š\n\n{text}"
        return self.generate_text(prompt, max_output_tokens=max_length*2)
    
    def generate_code(self, description: str, language: str = "Python") -> str:
        """ä»£ç ç”Ÿæˆ"""
        prompt = f"è¯·ç”¨{language}ç¼–å†™ä»£ç å®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š\n\n{description}\n\nè¯·æä¾›å®Œæ•´çš„ã€å¯è¿è¡Œçš„ä»£ç ï¼Œå¹¶åŒ…å«å¿…è¦çš„æ³¨é‡Šã€‚"
        return self.generate_text(prompt)
    
    def creative_writing(self, topic: str, style: str = "ç°ä»£æ•£æ–‡") -> str:
        """åˆ›æ„å†™ä½œ"""
        prompt = f"è¯·ä»¥{style}çš„é£æ ¼ï¼Œå†™ä¸€ç¯‡å…³äº'{topic}'çš„æ–‡ç« ã€‚"
        return self.generate_text(prompt)

def interactive_demo():
    """äº¤äº’å¼æ¼”ç¤º"""
    print("ğŸš€ Gemini API é«˜çº§åŠŸèƒ½æ¼”ç¤º")
    print("="*50)
    
    try:
        gemini = GeminiAdvanced()
        
        while True:
            print("\nğŸ“‹ è¯·é€‰æ‹©åŠŸèƒ½ï¼š")
            print("1. ğŸ“ æ–‡æœ¬ç”Ÿæˆ")
            print("2. ğŸ’¬ å¤šè½®å¯¹è¯")
            print("3. ğŸŒ æ–‡æœ¬ç¿»è¯‘")
            print("4. ğŸ“„ æ–‡æœ¬æ‘˜è¦")
            print("5. ğŸ’» ä»£ç ç”Ÿæˆ")
            print("6. âœï¸  åˆ›æ„å†™ä½œ")
            print("7. ğŸ–¼ï¸  å›¾ç‰‡åˆ†æ")
            print("8. ğŸ¨ å›¾ç‰‡ç”ŸæˆåŠ©æ‰‹")
            print("9. ğŸ“Š æŸ¥çœ‹å¯ç”¨æ¨¡å‹")
            print("10. âš™ï¸ æŸ¥çœ‹å½“å‰é…ç½®")
            print("0. ğŸšª é€€å‡º")
            
            choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (0-10): ").strip()
            
            if choice == "0":
                print("ğŸ‘‹ å†è§ï¼")
                break
            elif choice == "1":
                prompt = input("è¯·è¾“å…¥æ‚¨çš„æç¤ºï¼š")
                result = gemini.generate_text(prompt)
                print(f"\nğŸ”® ç”Ÿæˆç»“æœï¼š\n{result}")
            elif choice == "2":
                print("ğŸ’¬ å¼€å§‹å¤šè½®å¯¹è¯ (è¾“å…¥ 'quit' é€€å‡º)ï¼š")
                chat = gemini.start_chat()
                while True:
                    user_input = input("\næ‚¨ï¼š")
                    if user_input.lower() == 'quit':
                        break
                    response = gemini.chat_with_history(user_input, chat)
                    print(f"ğŸ¤–ï¼š{response}")
            elif choice == "3":
                text = input("è¯·è¾“å…¥è¦ç¿»è¯‘çš„æ–‡æœ¬ï¼š")
                lang = input("ç›®æ ‡è¯­è¨€ (é»˜è®¤ï¼šä¸­æ–‡)ï¼š") or "ä¸­æ–‡"
                result = gemini.translate_text(text, lang)
                print(f"\nğŸŒ ç¿»è¯‘ç»“æœï¼š\n{result}")
            elif choice == "4":
                text = input("è¯·è¾“å…¥è¦æ‘˜è¦çš„æ–‡æœ¬ï¼š")
                result = gemini.summarize_text(text)
                print(f"\nğŸ“„ æ‘˜è¦ç»“æœï¼š\n{result}")
            elif choice == "5":
                desc = input("è¯·æè¿°æ‚¨æƒ³è¦çš„åŠŸèƒ½ï¼š")
                lang = input("ç¼–ç¨‹è¯­è¨€ (é»˜è®¤ï¼šPython)ï¼š") or "Python"
                result = gemini.generate_code(desc, lang)
                print(f"\nğŸ’» ç”Ÿæˆçš„ä»£ç ï¼š\n{result}")
            elif choice == "6":
                topic = input("å†™ä½œä¸»é¢˜ï¼š")
                style = input("å†™ä½œé£æ ¼ (é»˜è®¤ï¼šç°ä»£æ•£æ–‡)ï¼š") or "ç°ä»£æ•£æ–‡"
                result = gemini.creative_writing(topic, style)
                print(f"\nâœï¸ åˆ›ä½œç»“æœï¼š\n{result}")
            elif choice == "7":
                image_path = input("è¯·è¾“å…¥å›¾ç‰‡è·¯å¾„ï¼š")
                prompt = input("åˆ†ææç¤º (é»˜è®¤ï¼šæè¿°è¿™å¼ å›¾ç‰‡)ï¼š") or "æè¿°è¿™å¼ å›¾ç‰‡"
                result = gemini.analyze_image(image_path, prompt)
                print(f"\nğŸ–¼ï¸ åˆ†æç»“æœï¼š\n{result}")
            elif choice == "8":
                print("ğŸ¨ å¯åŠ¨å›¾ç‰‡ç”ŸæˆåŠ©æ‰‹...")
                print("ğŸ’¡ æç¤ºï¼šå›¾ç‰‡ç”ŸæˆåŠ©æ‰‹å°†åœ¨æ–°çª—å£ä¸­è¿è¡Œ")
                try:
                    import subprocess
                    import sys
                    subprocess.run([sys.executable, "gemini_image_generation.py"])
                except Exception as e:
                    print(f"âŒ å¯åŠ¨å›¾ç‰‡ç”ŸæˆåŠ©æ‰‹å¤±è´¥ï¼š{e}")
                    print("è¯·æ‰‹åŠ¨è¿è¡Œï¼špython gemini_image_generation.py")
            elif choice == "9":
                gemini.list_available_models()
            elif choice == "10":
                gemini.show_config()
            else:
                print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")
    
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        print("è¯·ç¡®ä¿å·²è®¾ç½® GOOGLE_API_KEY ç¯å¢ƒå˜é‡")

if __name__ == "__main__":
    interactive_demo()
